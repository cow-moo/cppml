#include <iostream>
#include "linalg.hpp"
//#include "autodiff.hpp"
//#include "module.hpp"
//#include "solver.hpp"
//#include "dataloader.hpp"
//#include "loss.hpp"
#include "backend.hpp"
#include "timing.hpp"
#include "backend/cuda_buffer.hpp"
// #include <cuda_runtime.h>

using linalg::Tensor;
using linalg::Range;
using linalg::Shape;
// using autodiff::Expression;
// using autodiff::ComputationGraph;

int main() {
    // float* t;
    // cudaMalloc(&t, 10 * sizeof(float));
    // float buf[] = {1, 2, 3, 4, 5, 6, 7, 8, 9, 10};
    // cudaMemcpy(t, buf, 10 * sizeof(float), cudaMemcpyHostToDevice);
    // float buf2[10];
    // cudaMemcpy(buf2, t, 10 * sizeof(float), cudaMemcpyDeviceToHost);
    // for (size_t i = 0; i < 10; ++i)
    //     std::cout << buf2[i] << std::endl;
    // cudaFree(t);
    // return 0;

    Tensor<> t({1, 2, 3}, backend::BackendType::Cuda);
    t.print();
    std::cout << (float)t[0] << std::endl;
}

/*
TODO
- modules build the computation graph and hold weights/submodules between passes
- implement sum across abritrary axes
- expression class that holds n expression pointers
 - base case is expression that holds a tensor (Parameter)
 - forward pass sets up expression objects, computes final loss fn
 - backprop on loss function traverses the tree generated by expression classes and propagates
 - each expression must cache output as well as gradient of loss wrt output
 - 2 solutions for backprop order
  - compute how many "parent" expressions there are during graph construction. during DFS from loss expression, decrement count whenever expression is reached.
    then, only call backward() when count reaches zero
  - during graph construction, maintain linked list in topo sort order that splices lists from child expressions. then, during backprop iterate through list
 - use concept for expressions
  - value() -> Tensor& (computed and cached on construction)
  - gradient() -> Tensor* (nullptr if doesn't require gradient)
  - backward() (uses cached gradient here and propagates to childrens' gradient caches)
  - other fields
   - value cache (computed and initialized on construction)
   - gradient cache (if gradient is required, initialized to zero)
   - number of parents? or linked list of this expression and child expressions in topo order
 - actually lwk could make this template using this trick for forwards and backwards with pack arguments:
    constexpr auto lambda = [](int a) { return a * 2; }; 
    using U = T<lambda>;
    or T<decltype([](int a) { return a * 2; }){}> obj;

    template<auto forward, auto backward, typename Args...>
    class Expression {};

    template<typename U, typename V>
    AddExpr = Expression<[](Tensor& a, Tensor& b) { return a + b; }, [](Tensor& grad, Tensor* a, Tensor* b) { *a += grad, *b += grad; }, U, V>;
 - store list of learnable Parameters, zero out and use in expressions each time loss is computed. then optimizer does things with Parameter's value and gradient
 - how do we deal with rvalue lvalue? use universal reference T&& and forward() into some processing overload for rvalue and lvalue
  - rvalue should be moved into reference, lvalue should be saved
  - nvm doesnt work, rvalues cannot be moved into reference fields
 - make wrapper class Expression that contains Node pointer
  - allows stuff like x = x + y and means we don't have pointers to stack variables
- consider doing template <auto op> for passing functions. does not allow capture however
- simd: during axes recursion stop when subtensors are full (stored contiguously) and do simd operations
 - scalar operations should have built in operations to broadcast. research further
 - when broadcasting, do profiling for relative size between tensors. can try striding and adding or adding smaller tensor many times
 - think about when both tensors are being duplicated
 - could also stop earlier when theres a fixed stride (iff tail axes are fixed?)
 - use neon
- try to join consecutive operations? a + (b * 2) gets evaluated once
- random init
- implement tail recursion elimination
- research how to optimize/parallelize these elementwise operations
- .sum()
- figure out scalar operations
- matmul
- reshape
 - might need to do copy operations when slices occur
 - in place or return?
- numpy also does copies when doing arbitrary slices, seems to also keep views with Range objects
- figure out elementwise assignment (make TensorSlice class with elementwise assignment and Tensor conversion operator for implicit casting?)
- idea: make the TensorSlice class just modify indexing in place? (doesn't really work)
- other idea: have elementwise operation just keep track of running index offsets so it doesn't need to make new Tensor slices
 - probably more correct
- main problem is that we're copying axisIndices/shape which mostly aren't changing and shared ptr data overhead?
*/